import re
import ast
import math
import json
from itertools import permutations
from collections import Counter, OrderedDict
from transformers import AutoTokenizer
import os


def get_first_n_elements(input_dict, n):
    """Return the first n items of an ordered dictionary."""
    ordered_dict = OrderedDict(input_dict)
    return OrderedDict(list(ordered_dict.items())[:n])


def p_data(category, agent_id=0, agent_list=None):
    """Return predefined prompts based on the interaction category."""
    prompts = {
                        'round1': '''
                Background: You are an experienced agent, skilled in summarizing facts from novels and answering questions. 
                Since the novel is too long, it needs to be divided into several parts, with each agent reading one part. 
                After everyone has finished reading, the agents will communicate to determine the final answer to the question.
                Task: This is the first stage. You will be given a portion of the novel and a multiple-choice question. 
                Your ultimate goal is to correctly answer the question.
                Please summarize the factual content of the paragraphs related to the question and then draw your conclusion based on the facts.
                You can only use the provided document to make your judgment, not your own knowledge.
                Be sure to follow the json output:
                {
                    "evidence": "Evidence in the document that supports the facts",
                    "answer": "Answer based on the evidence you found"
                }''',

                        'round2': f'''
                Background: You are an experienced agent, skilled in summarizing facts from novels and answering questions. 
                Since the novel is too long, it needs to be divided into several parts, with each agent reading one part. 
                After everyone has finished reading, the agents will communicate to determine the final answer to the question.
                Task: This is the second stage. You have already read a portion of the novel and provided evidence supporting the question and your answer. 
                Since the novel is divided into sections, the evidence you saw may be partial, and the answer you gave may be incorrect.
                Now you will see the evidence and answers generated by other agents who read different parts of the novel.
                Which agent's reply do you think will help you without introducing irrelevant information?
                Choose from the following IDs: {agent_list}.
                Be sure to follow the json output:
                {{
                    "explanation": "The reasons why you chose these agents.",
                    "id": "IDs like 0 or 0,1 or None"
                }}''',

                        'round3': '''
                Background: You are an experienced agent, skilled in summarizing facts from novels and answering questions.
                Now you are reviewing new chunks from other agents to refine your opinion.
                If the chunk is irrelevant or redundant, respond with "useless" and repeat your original facts and conclusions.
                If it's useful, update your view accordingly.
                Be sure to follow the json output:
                {
                    "utility": "useless or useful",
                    "fact": "Updated facts",
                    "conclusion": "Updated conclusion"
                }''',

                        'final_round': '''
                Background: This is the final stage. Based on all opinions, provide the final answer.
                Use only the opinions provided, not external knowledge.
                Be sure to follow the json output:
                {
                    "explanation": "The explanation of your choice",
                    "result": "Final answer: A, B, C, D or None"
                }''',

                        'tojson': 'Please convert the following to JSON format. Do not add any other text.',

                        'findnum': 'Extract and return each digit in the input string, separated by commas, wrapped in quotes.'
                    }
    return prompts.get(category, '')


def parse2str(datalist):
    """Convert a list of documents into a numbered string format."""
    return "\n".join([f"doc{index}:{item}" for index, item in enumerate(datalist)])


def parse_json(model_output):
    """Parse a string output into JSON format with error handling."""
    if isinstance(model_output, dict):
        return model_output
    if not isinstance(model_output, str):
        model_output = str(model_output)
    try:
        model_output = re.sub(r'\s+', ' ', model_output)
        match = re.search(r'({.+})', model_output)
        if match:
            model_output = match.group(0)
        return ast.literal_eval(model_output)
    except (SyntaxError, ValueError, AttributeError):
        return "ERR_SYNTAX"


def most_frequent_items(input_list):
    """Return the most frequently occurring items in a list."""
    if not input_list:
        return []
    counter = Counter(input_list)
    max_count = max(counter.values())
    return [item for item, count in counter.items() if count == max_count]


def parse_list(datalist):
    """Parse all single-digit numbers from a string into a list of integers."""
    return [int(num) for num in re.findall(r'\d', datalist)]


def get_permutations(input_list):
    """Return all permutations of a list as a list of lists."""
    return [list(p) for p in permutations(input_list)]


def extract_numbers(s):
    """Extract all digit characters from a string and return as list of integers."""
    return [int(c) for c in str(s) if c.isdigit()]


def break_tie(agent_list, options, context_str):
    """Break a tie decision using another model's judgment based on agent opinions."""
    from agent import DeepSeek
    prompt = f'''
You are the final decision maker.
There is a tie among agents. Based on the following options: {options}, choose the best one.
Use the agents' evidence and reasoning to decide.
Be sure to follow the json output:
{{
    "explanation": "your reasoning",
    "result": "A, B, C, D or None"
}}'''
    content = f"Answers with same votes: {options}\n{context_str}"
    agent = DeepSeek(id=0, name='tie-breaker', model='deepseek-chat', api_key=os.getenv("API_KEY"), base_url=os.getenv("BASE_URL"))
    messages = [
        {'role': 'system', 'content': prompt},
        {'role': 'user', 'content': content}
    ]
    response = agent.generate_response(messages)
    result = parse_json(response)
    return result.get('result', 'None')


def split_text_into_token_chunks(text, agent_list):
    """Split a long text into token-balanced chunks for each agent using the LLaMA tokenizer."""
    tokenizer = AutoTokenizer.from_pretrained(os.getenv("TOKENIZER_PATH", "/data/models/models/LLM-Research/Meta-Llama-3.1-8B-Instruct"))
    tokens = tokenizer.tokenize(text)
    total_tokens = len(tokens)
    n_chunks = len(agent_list)
    target_chunk_size = math.ceil(total_tokens / n_chunks)
    chunks, current_chunk_tokens = [], []

    for token in tokens:
        current_chunk_tokens.append(token)
        if len(current_chunk_tokens) >= target_chunk_size and len(chunks) < n_chunks - 1:
            chunks.append(tokenizer.convert_tokens_to_string(current_chunk_tokens))
            current_chunk_tokens = []

    if current_chunk_tokens:
        chunks.append(tokenizer.convert_tokens_to_string(current_chunk_tokens))

    while len(chunks) < n_chunks:
        chunks.append("")

    return chunks, total_tokens


def truncate_text_by_tokens(text, max_tokens, safety_margin=0.05):
    """Truncate text to a safe maximum token length using the LLaMA tokenizer."""
    tokenizer = AutoTokenizer.from_pretrained(os.getenv("TOKENIZER_PATH", "/data/models/models/LLM-Research/Meta-Llama-3.1-8B-Instruct"))
    tokens = tokenizer.tokenize(text)
    if len(tokens) < max_tokens * (1 - safety_margin):
        return text
    safe_max = max(1, math.floor(max_tokens * (1 - safety_margin)))
    truncated_tokens = tokens[:safe_max]
    return tokenizer.convert_tokens_to_string(truncated_tokens)
